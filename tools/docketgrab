#!/usr/bin/env python

# Copyright (c) 2018  Floyd Terbo

import argparse
import json
import logging
import os
import os.path
import sys
import time
import urllib

import BeautifulSoup as BS
import requests

logging.basicConfig(level=logging.INFO)

HEADERS = {"User-Agent" : "SCOTUS Docket Grabber (https://github.com/fterbo/scotus-tools)"}
BASE = "https://www.supremecourt.gov/rss/cases/JSON/"

PETITION_LINKS = set(["Petition", "Appendix", "Jurisdictional Statement"])
PETITION_TYPES = set(["certiorari", "mandamus", "habeas", "jurisdiction"])

def GET (url):
  logging.debug("GET: %s" % (url))
  return requests.get(url, headers=HEADERS)


def parse_args ():
  parser = argparse.ArgumentParser()
  parser.add_argument("-t", "--term", dest="term", type=int)
  parser.add_argument("-n", "--docket-num", dest="docket_num", type=int)
  parser.add_argument("-a", "--action", dest="action", type=str, default="scan-petition")
  parser.add_argument("--stop", dest="stop", type=int, default=0)
  parser.add_argument("--root", dest="root", type=str, default=".")
  args = parser.parse_args()
  return args


# TODO: We really should use some exceptions to better handle control flow
def scanPetitions (path, opts):
  nextnum = opts.docket_num
  while True:
    newpath = "%s/%d/" % (path, nextnum)
    docketstr = "%d-%d" % (opts.term, nextnum)

    if os.path.exists("%s/docket.json" % (newpath)):
      # We already scanned this docket
      nextnum += 1
      continue

    url = "%s/%d-%d.json" % (BASE, opts.term, nextnum)
    r = GET(url)
    if r.status_code != 200:
      if opts.stop and nextnum < opts.stop:
        logging.info("Skipping <%s> with no docket" % (url))
        nextnum += 1
        continue
      else:
        logging.info("Stopped for URL <%s> with code %d" % (url, r.status_code))
        break

    docket_obj = r.json()
    founditem = None
    for item in docket_obj["ProceedingsandOrder"]:
      try:
        for link in item["Links"]:
          if link["Description"] == "Petition":
            # TODO: This does not tend to capture original actions or mandatory appeals
            founditem = item
            break
        if founditem:
          break
      except KeyError:
        # Likely original extension of time to file
        logging.debug("[%s] No links: %s" % (docketstr, item["Text"]))
        continue

    try:
      os.makedirs(newpath)
    except OSError:
      pass # Should check errno, but probably already exists

    with open("%s/docket.json" % (newpath), "w+") as f:
      f.write(json.dumps(r.json()))

    if not founditem:
      logging.error("Couldn't find a petition for docket %s" % (docketstr))
      nextnum += 1
      continue

    match = list(set(founditem["Text"].split()) & PETITION_TYPES)
    casename = ""
    try:
      # TODO: This is all horrible, but works for most cases
      if match[0] in ["mandamus", "habeas"]:
        casename = docket_obj["PetitionerTitle"]
      else:
        petitioner = docket_obj["PetitionerTitle"][:-12]  # Remove ", Petitioner" from metadata
        casename = "%s v. %s" % (petitioner, docket_obj["RespondentTitle"])
    except Exception:
      logging.exception("Unable to create case name for %s" % (docketstr))

    if len(match) == 0:
      logging.warning("[%s] Unknown petition type for: %s" % (docketstr, founditem["Text"]))
    elif len(match) == 1:
      logging.info("[%s] (%s) %s" % (docketstr, match[0], casename))
    elif len(match) > 1:
      logging.info("[%s] Too many type matches for: %s" % (docketstr, founditem["Text"]))

    for link in founditem["Links"]:
      if link["Description"] in PETITION_LINKS:
        logging.debug("[%s] Downloading %s" % (docketstr, link["File"]))
        dl = GET(link["DocumentUrl"])
        if dl.status_code != 200:
          logging.error("[%s] FAILED: %d" % (docketstr, dl.status_code))
          continue
        outpath = "%s/%s" % (newpath, link["File"])
        with open(outpath, "w+") as f:
          f.write(dl.content)

    nextnum += 1
        

# TODO: we should optionally allow checking last-modified with HEAD before we grab if the caller
# is doing a larger scan where many are unlikely to be updated since last scan
def downloadFull (path, opts):
  docketstr = "%d-%d" % (opts.term, opts.docket_num)
  newpath = "%s/%d/" % (path, opts.docket_num)
  try:
    os.makedirs(newpath)
  except OSError:
    pass

  url = "%s/%d-%d.json" % (BASE, opts.term, opts.docket_num)
  r = GET(url)
  if r.status_code != 200:
    logging.error("Could not get docket metadata <%s> with code %d" % (url, r.status_code))
    sys.exit(1)

  docket_obj = r.json()
  with open("%s/docket.json" % (newpath), "w+") as df:
    df.write(json.dumps(docket_obj))

  for item in docket_obj["ProceedingsandOrder"]:
    if "Links" not in item:
      continue
    for link in item["Links"]:
      if "DocumentUrl" not in link:
        logging.warning("Found link without DocumentUrl: %s" % (link["Description"]))
        continue
      fname = urllib.unquote_plus(link["DocumentUrl"].split("/")[-1])
      logging.debug("[%s] Downloading %s" % (docketstr, fname))
      dl = GET(link["DocumentUrl"])
      if dl.status_code != 200:
        logging.error("[%s] FAILED: %d" % (docketstr, dl.status_code))
        continue
      outpath = "%s/%s" % (newpath, fname)
      with open(outpath, "w+") as f:
        f.write(dl.content)


ACTIONS = {
  "scan-petition" : scanPetitions,
  "petitions" : scanPetitions,
  "download-full" : downloadFull,
  "download" : downloadFull
}

if __name__ == '__main__':
  opts = parse_args()
  path = "%s/OT-%d/dockets" % (opts.root, opts.term)
  try:
    os.makedirs(path)
  except OSError:
    pass

  func = ACTIONS[opts.action]
  func(path, opts)
